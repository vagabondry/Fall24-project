{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to understand how to use transformers yay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.45.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: datasets in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: torch in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.34.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.10.9)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: psutil in c:\\users\\nika\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\nika\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nika\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                                  0  \\\n",
      "0           0            What kind of phone(s) do you guys have?   \n",
      "1           1  I have a pixel. It's pretty great. Much better...   \n",
      "2           2       Does it really charge all the way in 15 min?   \n",
      "3           3            What kind of phone(s) do you guys have?   \n",
      "4           4  Samsung Galaxy J1. It's my first cell phone an...   \n",
      "\n",
      "                                                   1  \\\n",
      "0  I have a pixel. It's pretty great. Much better...   \n",
      "1       Does it really charge all the way in 15 min?   \n",
      "2  Pretty fast. I've never timed it, but it's und...   \n",
      "3  Samsung Galaxy J1. It's my first cell phone an...   \n",
      "4  What do you think of it? Anything you don't like?   \n",
      "\n",
      "                                                   2  \n",
      "0       Does it really charge all the way in 15 min?  \n",
      "1  Pretty fast. I've never timed it, but it's und...  \n",
      "2  cool. I've been thinking of getting one, my ph...  \n",
      "3  What do you think of it? Anything you don't like?  \n",
      "4  I love it. I can't think of anything I don't l...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./archive/casual_data_windows.csv')\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    What kind of phone(s) do you guys have? [SEP] ...\n",
      "1    I have a pixel. It's pretty great. Much better...\n",
      "2    Does it really charge all the way in 15 min? [...\n",
      "3    What kind of phone(s) do you guys have? [SEP] ...\n",
      "4    Samsung Galaxy J1. It's my first cell phone an...\n",
      "Name: formatted_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def format_conversation(row):\n",
    "    if pd.notna(row['2']):\n",
    "        return f\"{row['0']} [SEP] {row['1']} [SEP] {row['2']}\"\n",
    "    else:\n",
    "        return f\"{row['0']} [SEP] {row['1']}\"\n",
    "\n",
    "df['formatted_text'] = df.apply(format_conversation, axis=1)\n",
    "\n",
    "print(df['formatted_text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['formatted_text'], return_special_tokens_mask=True, truncation=True, padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_datasets = df['formatted_text'].apply(lambda x: tokenizer(x, truncation=True, padding='max_length', max_length=512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        self.input_ids = [torch.tensor(t['input_ids']) for t in tokenized_texts]\n",
    "        self.attn_masks = [torch.tensor(t['attention_mask']) for t in tokenized_texts]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attn_masks[idx],\n",
    "            'labels': self.input_ids[idx],  \n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = ConversationDataset(tokenized_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4983ea830645c6bea831e8c5fc70d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42225 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     24\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     26\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     27\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     28\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,  \u001b[38;5;66;03m# Can split for validation if needed\u001b[39;00m\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Fine-tune the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2394\u001b[0m ):\n\u001b[0;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\Nika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\trainer.py:3518\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3518\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\Nika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\accelerate\\accelerator.py:2196\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2195\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2196\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Nika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\", \n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset,  \n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm your AI chatbot. How can I help you today?\n",
      "Chatbot: I'm sorry, I didn't understand that. Could you rephrase?\n",
      "Chatbot: I'm just a bot, but thanks for asking! How about you?\n",
      "Chatbot: I'm sorry, I didn't understand that. Could you rephrase?\n",
      "Chatbot: I'm sorry, I didn't understand that. Could you rephrase?\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "dataset = {\n",
    "    \"hello\": \"Hello! How can I help you?\",\n",
    "    \"how are you\": \"I'm just a bot, but thanks for asking! How about you?\",\n",
    "    \"what's your name\": \"I'm an AI chatbot. You can call me Bot!\",\n",
    "    \"what is the time\": \"I'm sorry, I can't tell time. But you can check your system clock!\",\n",
    "    \"bye\": \"Goodbye! Have a nice day!\"\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "questions = list(dataset.keys())\n",
    "\n",
    "X = vectorizer.fit_transform(questions)\n",
    "\n",
    "def get_response(user_input):\n",
    "    user_input_vector = vectorizer.transform([user_input])\n",
    "\n",
    "    similarities = cosine_similarity(user_input_vector, X).flatten()\n",
    "\n",
    "    best_match_index = np.argmax(similarities)\n",
    "\n",
    "    if similarities[best_match_index] < 0.2:  \n",
    "        return \"I'm sorry, I didn't understand that. Could you rephrase?\"\n",
    "    \n",
    "    # Return the best response from the dataset\n",
    "    best_match_question = questions[best_match_index]\n",
    "    return dataset[best_match_question]\n",
    "\n",
    "def chatbot():\n",
    "    print(\"Hello! I'm your AI chatbot. How can I help you today?\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"You: \").lower()\n",
    "        \n",
    "        if user_input in [\"bye\", \"exit\", \"quit\"]:\n",
    "            print(\"Chatbot: Goodbye! Have a great day!\")\n",
    "            break\n",
    "        \n",
    "        response = get_response(user_input)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nika\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_11', 'keras_tensor_16']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m704/704\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1498s\u001b[0m 2s/step - loss: 1.8053 - val_loss: 0.7181\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<start>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 121\u001b[0m\n\u001b[0;32m    119\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([input_sentence])\n\u001b[0;32m    120\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequence, maxlen\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Generate empty target sequence of length 1\u001b[39;00m\n\u001b[0;32m     90\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 91\u001b[0m target_seq[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<start>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# start token\u001b[39;00m\n\u001b[0;32m     93\u001b[0m stop_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     94\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: '<start>'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "\n",
    "data = pd.read_csv('./archive/casual_data_windows.csv')\n",
    "\n",
    "data.head()\n",
    "data['0'] = data['0'].fillna('')\n",
    "data['1'] = data['1'].fillna('')\n",
    "\n",
    "# Extract conversations\n",
    "utterances_A = data['0'].values\n",
    "utterances_B = data['1'].values\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(list(utterances_A) + list(utterances_B))\n",
    "\n",
    "utterances_A_seq = tokenizer.texts_to_sequences(utterances_A)\n",
    "utterances_B_seq = tokenizer.texts_to_sequences(utterances_B)\n",
    "\n",
    "max_length = max([len(seq) for seq in utterances_A_seq])\n",
    "utterances_A_seq = pad_sequences(utterances_A_seq, maxlen=max_length, padding='post')\n",
    "utterances_B_seq = pad_sequences(utterances_B_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "latent_dim = 256  \n",
    "embedding_dim = 100 \n",
    "\n",
    "encoder_inputs = Input(shape=(max_length,))\n",
    "encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "encoder_lstm, state_h, state_c = LSTM(latent_dim, return_state=True)(encoder_embedding)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(max_length,))\n",
    "decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "decoder_target_data = np.expand_dims(utterances_B_seq, -1)\n",
    "\n",
    "model.fit([utterances_A_seq, utterances_B_seq], decoder_target_data, batch_size=64, epochs=1, validation_split=0.2)\n",
    "\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def generate_response(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1))\n",
    "    target_seq[0, 0] = tokenizer.word_index['<start>']\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer.index_word[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "        if sampled_word == '<end>' or len(decoded_sentence) > max_length:\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "        \n",
    "        states_value = [h, c]\n",
    "    \n",
    "    return decoded_sentence\n",
    "\n",
    "input_sentence = \"Hello, how are you?\"\n",
    "input_sequence = tokenizer.texts_to_sequences([input_sentence])\n",
    "input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding='post')\n",
    "response = generate_response(input_sequence)\n",
    "print(\"Chatbot response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'<start>'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences([input_sentence])\n\u001b[0;32m      3\u001b[0m input_sequence \u001b[38;5;241m=\u001b[39m pad_sequences(input_sequence, maxlen\u001b[38;5;241m=\u001b[39mmax_length, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatbot response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[1;32mIn[6], line 91\u001b[0m, in \u001b[0;36mgenerate_response\u001b[1;34m(input_seq)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Generate empty target sequence of length 1\u001b[39;00m\n\u001b[0;32m     90\u001b[0m target_seq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 91\u001b[0m target_seq[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_index\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<start>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# start token\u001b[39;00m\n\u001b[0;32m     93\u001b[0m stop_condition \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     94\u001b[0m decoded_sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mKeyError\u001b[0m: '<start>'"
     ]
    }
   ],
   "source": [
    "input_sentence = \"Hello, how are you?\"\n",
    "input_sequence = tokenizer.texts_to_sequences([input_sentence])\n",
    "input_sequence = pad_sequences(input_sequence, maxlen=max_length, padding='post')\n",
    "response = generate_response(input_sequence)\n",
    "print(\"Chatbot response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
