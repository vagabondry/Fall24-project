{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":986563,"sourceType":"datasetVersion","datasetId":540134}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:01:31.456874Z","iopub.execute_input":"2024-10-26T11:01:31.457577Z","iopub.status.idle":"2024-10-26T11:01:44.267973Z","shell.execute_reply.started":"2024-10-26T11:01:31.457540Z","shell.execute_reply":"2024-10-26T11:01:44.266918Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install peft\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:01:49.015178Z","iopub.execute_input":"2024-10-26T11:01:49.015554Z","iopub.status.idle":"2024-10-26T11:02:01.486431Z","shell.execute_reply.started":"2024-10-26T11:01:49.015518Z","shell.execute_reply":"2024-10-26T11:02:01.485415Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.2)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.4.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.45.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.34.2)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.5)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.25.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2024.5.15)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: peft\nSuccessfully installed peft-0.13.2\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\nfrom peft import LoraConfig, get_peft_model\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:02:10.059226Z","iopub.execute_input":"2024-10-26T11:02:10.059613Z","iopub.status.idle":"2024-10-26T11:02:15.306735Z","shell.execute_reply.started":"2024-10-26T11:02:10.059575Z","shell.execute_reply":"2024-10-26T11:02:15.305931Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\ntokenizer.add_special_tokens({'sep_token': '[SEP]'})\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:02:29.935850Z","iopub.execute_input":"2024-10-26T11:02:29.937063Z","iopub.status.idle":"2024-10-26T11:02:35.738340Z","shell.execute_reply.started":"2024-10-26T11:02:29.937018Z","shell.execute_reply":"2024-10-26T11:02:35.737230Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3c933aacaf347b4afec574829fe7edd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44cc4c74a5f9431c826bc9c5c01bcc8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8fec593b77b4bddac93a96025679a4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5215f3f514e74116808152403ca1eac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c489b2e08df9426093318b3092652930"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"model = GPT2LMHeadModel.from_pretrained('distilgpt2')\nmodel.resize_token_embeddings(len(tokenizer))\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=4,                      \n    lora_alpha=32,            \n    lora_dropout=0.1,         \n    target_modules=[\"c_attn\"] \n)\n\nmodel = get_peft_model(model, lora_config)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:02:39.476624Z","iopub.execute_input":"2024-10-26T11:02:39.477463Z","iopub.status.idle":"2024-10-26T11:02:57.151353Z","shell.execute_reply.started":"2024-10-26T11:02:39.477421Z","shell.execute_reply":"2024-10-26T11:02:57.150337Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c27321b25b934cb39e3459a486bf4a2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5081f5d1fd2b4487816b91d24143f9ef"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Dataset tokenization","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('/kaggle/input/reddit-conversations/casual_data_windows.csv')\n\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:03:02.568742Z","iopub.execute_input":"2024-10-26T11:03:02.569228Z","iopub.status.idle":"2024-10-26T11:03:02.878307Z","shell.execute_reply.started":"2024-10-26T11:03:02.569177Z","shell.execute_reply":"2024-10-26T11:03:02.876913Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"   Unnamed: 0                                                  0  \\\n0           0            What kind of phone(s) do you guys have?   \n1           1  I have a pixel. It's pretty great. Much better...   \n2           2       Does it really charge all the way in 15 min?   \n3           3            What kind of phone(s) do you guys have?   \n4           4  Samsung Galaxy J1. It's my first cell phone an...   \n\n                                                   1  \\\n0  I have a pixel. It's pretty great. Much better...   \n1       Does it really charge all the way in 15 min?   \n2  Pretty fast. I've never timed it, but it's und...   \n3  Samsung Galaxy J1. It's my first cell phone an...   \n4  What do you think of it? Anything you don't like?   \n\n                                                   2  \n0       Does it really charge all the way in 15 min?  \n1  Pretty fast. I've never timed it, but it's und...  \n2  cool. I've been thinking of getting one, my ph...  \n3  What do you think of it? Anything you don't like?  \n4  I love it. I can't think of anything I don't l...  \n","output_type":"stream"}]},{"cell_type":"code","source":"def format_conversation(row):\n    if pd.notna(row['2']):\n        return f\"{row['0']} [SEP] {row['1']} [SEP] {row['2']}\"\n    else:\n        return f\"{row['0']} [SEP] {row['1']}\"\n\ndf['formatted_text'] = df.apply(format_conversation, axis=1)\n\nprint(df['formatted_text'].head())\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:04:21.564859Z","iopub.execute_input":"2024-10-26T11:04:21.565897Z","iopub.status.idle":"2024-10-26T11:04:22.997735Z","shell.execute_reply.started":"2024-10-26T11:04:21.565841Z","shell.execute_reply":"2024-10-26T11:04:22.996672Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"0    What kind of phone(s) do you guys have? [SEP] ...\n1    I have a pixel. It's pretty great. Much better...\n2    Does it really charge all the way in 15 min? [...\n3    What kind of phone(s) do you guys have? [SEP] ...\n4    Samsung Galaxy J1. It's my first cell phone an...\nName: formatted_text, dtype: object\n","output_type":"stream"}]},{"cell_type":"code","source":"def tokenize_function(text):\n    return tokenizer(text, return_special_tokens_mask=True, truncation=True, padding='max_length', max_length=512)\n\n# Tokenize the dataset\ntokenized_datasets = df['formatted_text'].apply(tokenize_function)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:04:28.680076Z","iopub.execute_input":"2024-10-26T11:04:28.680461Z","iopub.status.idle":"2024-10-26T11:04:57.036640Z","shell.execute_reply.started":"2024-10-26T11:04:28.680423Z","shell.execute_reply":"2024-10-26T11:04:57.035734Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class ConversationDataset(Dataset):\n    def __init__(self, tokenized_texts):\n        self.input_ids = [torch.tensor(t['input_ids']) for t in tokenized_texts]\n        self.attention_masks = [torch.tensor(t['attention_mask']) for t in tokenized_texts]\n\n    def __len__(self):\n        return len(self.input_ids)\n\n    def __getitem__(self, idx):\n        return {\n            'input_ids': self.input_ids[idx],\n            'attention_mask': self.attention_masks[idx],\n            'labels': self.input_ids[idx],  \n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:05:00.663473Z","iopub.execute_input":"2024-10-26T11:05:00.663862Z","iopub.status.idle":"2024-10-26T11:05:00.671880Z","shell.execute_reply.started":"2024-10-26T11:05:00.663825Z","shell.execute_reply":"2024-10-26T11:05:00.670985Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_dataset = ConversationDataset(tokenized_datasets)\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:05:09.077908Z","iopub.execute_input":"2024-10-26T11:05:09.078838Z","iopub.status.idle":"2024-10-26T11:05:29.596265Z","shell.execute_reply.started":"2024-10-26T11:05:09.078795Z","shell.execute_reply":"2024-10-26T11:05:29.595310Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): GPT2LMHeadModel(\n      (transformer): GPT2Model(\n        (wte): Embedding(50258, 768)\n        (wpe): Embedding(1024, 768)\n        (drop): Dropout(p=0.1, inplace=False)\n        (h): ModuleList(\n          (0-5): 6 x GPT2Block(\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (attn): GPT2SdpaAttention(\n              (c_attn): lora.Linear(\n                (base_layer): Conv1D(nf=2304, nx=768)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (c_proj): Conv1D(nf=768, nx=768)\n              (attn_dropout): Dropout(p=0.1, inplace=False)\n              (resid_dropout): Dropout(p=0.1, inplace=False)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): GPT2MLP(\n              (c_fc): Conv1D(nf=3072, nx=768)\n              (c_proj): Conv1D(nf=768, nx=3072)\n              (act): NewGELUActivation()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=2e-5)\nnum_epochs = 5\ntotal_steps = len(train_dataloader) * num_epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:05:35.428791Z","iopub.execute_input":"2024-10-26T11:05:35.429233Z","iopub.status.idle":"2024-10-26T11:05:35.876664Z","shell.execute_reply.started":"2024-10-26T11:05:35.429187Z","shell.execute_reply":"2024-10-26T11:05:35.875905Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"model.train()\nfor epoch in range(num_epochs):\n    for batch in tqdm(train_dataloader):\n        optimizer.zero_grad()\n        inputs = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n\n        optimizer.step()\n        scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-10-26T11:05:44.959889Z","iopub.execute_input":"2024-10-26T11:05:44.960534Z","iopub.status.idle":"2024-10-26T16:38:28.472957Z","shell.execute_reply.started":"2024-10-26T11:05:44.960489Z","shell.execute_reply":"2024-10-26T16:38:28.471918Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"100%|██████████| 14075/14075 [1:06:27<00:00,  3.53it/s]\n100%|██████████| 14075/14075 [1:06:34<00:00,  3.52it/s]\n100%|██████████| 14075/14075 [1:06:33<00:00,  3.52it/s]\n100%|██████████| 14075/14075 [1:06:33<00:00,  3.52it/s]\n100%|██████████| 14075/14075 [1:06:34<00:00,  3.52it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained(\"lora_fine_tuned_gpt2\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T16:44:57.751553Z","iopub.execute_input":"2024-10-26T16:44:57.752409Z","iopub.status.idle":"2024-10-26T16:45:00.103837Z","shell.execute_reply.started":"2024-10-26T16:44:57.752366Z","shell.execute_reply":"2024-10-26T16:45:00.103043Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Evaluation","metadata":{}},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\nfrom peft import LoraConfig, get_peft_model\n\n# Load the tokenizer\n#tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n#tokenizer.add_special_tokens({'sep_token': '[SEP]'})\n#tokenizer.pad_token = tokenizer.eos_token\n\n# Load the fine-tuned model\n#model = GPT2LMHeadModel.from_pretrained(\"lora_fine_tuned_gpt2\")\n#model.resize_token_embeddings(len(tokenizer))\nmodel.eval()  # Set model to evaluation mode for inference\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T16:40:32.127946Z","iopub.execute_input":"2024-10-26T16:40:32.128334Z","iopub.status.idle":"2024-10-26T16:40:32.138213Z","shell.execute_reply.started":"2024-10-26T16:40:32.128299Z","shell.execute_reply":"2024-10-26T16:40:32.137043Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"PeftModel(\n  (base_model): LoraModel(\n    (model): GPT2LMHeadModel(\n      (transformer): GPT2Model(\n        (wte): Embedding(50258, 768)\n        (wpe): Embedding(1024, 768)\n        (drop): Dropout(p=0.1, inplace=False)\n        (h): ModuleList(\n          (0-5): 6 x GPT2Block(\n            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (attn): GPT2SdpaAttention(\n              (c_attn): lora.Linear(\n                (base_layer): Conv1D(nf=2304, nx=768)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=768, out_features=4, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=4, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (c_proj): Conv1D(nf=768, nx=768)\n              (attn_dropout): Dropout(p=0.1, inplace=False)\n              (resid_dropout): Dropout(p=0.1, inplace=False)\n            )\n            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (mlp): GPT2MLP(\n              (c_fc): Conv1D(nf=3072, nx=768)\n              (c_proj): Conv1D(nf=768, nx=3072)\n              (act): NewGELUActivation()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n        )\n        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (lm_head): Linear(in_features=768, out_features=50258, bias=False)\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"code","source":"import torch\n\ndef generate_response(prompt, model, tokenizer, max_length=50):\n    # Encode the prompt\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    \n    # Move inputs to GPU if available\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    inputs = {key: val.to(device) for key, val in inputs.items()}\n    model.to(device)\n\n    # Generate response\n    output = model.generate(\n        **inputs,\n        max_length=max_length,\n        num_return_sequences=1,\n        no_repeat_ngram_size=2,  # Prevent repetition of phrases\n        top_k=50,                # Use top-k sampling to reduce randomness\n        top_p=0.9,               # Use nucleus sampling for diversity\n        temperature=0.7          # Lower values make the output less random\n    )\n\n    # Decode the generated tokens back to text\n    response = tokenizer.decode(output[0], skip_special_tokens=True)\n    return response\n\n# Test with a sample prompt\nprompt = \"you are not okay at all, you dumb tincan.\"\nresponse = generate_response(prompt, model, tokenizer)\nprint(\"Model response:\", response)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T16:43:59.980451Z","iopub.execute_input":"2024-10-26T16:43:59.980838Z","iopub.status.idle":"2024-10-26T16:44:00.123120Z","shell.execute_reply.started":"2024-10-26T16:43:59.980800Z","shell.execute_reply":"2024-10-26T16:44:00.122231Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Model response: you are not okay at all, you dumb tincan.  I'm not a fan of the game. I just want to play it.\n","output_type":"stream"}]}]}